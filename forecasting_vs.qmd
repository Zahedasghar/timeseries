---
title: "[Stationary and Nonstationary Series]{.flow}"
subtitle: "Macroeconomic Forecasting Lecture"
author: "[[Zahid Asghar, Professor, School of Economics, QAU](https://zahedasghar.netlify.app)]{style=color:blue;}"
date: "today"
format:
  revealjs:
    theme: [default]
    chalkboard: true
    slide-number: c/t
execute:
  freeze: auto
  error: true
---

# Properties of Time Series

## Outline

### Part 1 : Stationary processes

- Identification
- Estimation & Model Selection
- Putting it all together

### Part 2: Nonstationary process

- Characterization
- Testing

---

## Univariate Analysis

![Probability Density](images/prob_density.png){align="center"}

Each distribution is a draw from a **random process**.

---

## Strong Stationarity {.scrollable}

- **Strong Stationarity vs. Weak (Covariance) Stationarity**  
  - **Strong Stationarity**:
    1. Same distribution over time  
    2. Same covariance structure over time  
    (A theoretical concept; in practice, we use covariance stationarity.)
  - **Covariance Stationarity**:
    1. Unconditional mean \(E(Y_t) = E(Y_{t+j}) = \mu\)  
    2. Constant variance \(Var(Y_t) = Var(Y_{t+j}) = \sigma^2_y\)  
    3. Covariance depends only on the lag \(j\):  
       \[
       Cov(Y_t,Y_{t+j}) = Cov(Y_s,Y_{s+j}) = \gamma_j
       \]

---

## Part 1 : Stationary Process

Just to remind you:

- Identification  
- Estimation & Model Selection  
- Putting it all together

The first step is visual inspection—graph and observe your data.

> “You can observe a lot just by watching.” – Yogi Berra

---

## Plot Your Data

Below are Python examples that simulate and plot various time series models.

### AR(1) Model Simulation

```{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.tsa.arima_process import ArmaProcess

np.random.seed(123)
phi = 0.8  # AR coefficient
n = 200    # Number of time points



# For an AR(1) process, the AR lag polynomial is [1, -phi]
ar_params = np.array([1, -phi])
ma_params = np.array([1])  # No MA part
ar1_process = ArmaProcess(ar_params, ma_params)
ar1_data = ar1_process.generate_sample(nsample=n)

# Create DataFrame and plot
df_ar1 = pd.DataFrame({'Time': np.arange(1, n+1), 'Value': ar1_data})
plt.figure(figsize=(8, 4))
plt.plot(df_ar1['Time'], df_ar1['Value'], linewidth=1)
plt.title("AR(1) Model")
plt.xlabel("Time")
plt.ylabel("Value")
plt.tight_layout()
plt.show()
```

### MA(1) Model Simulation

```{python}
np.random.seed(123)
theta = 0.6  # MA coefficient
ma_params = np.array([1, theta])  # Note: statsmodels expects the sign as given for MA
ar_params = np.array([1])         # No AR part
ma1_process = ArmaProcess(ar_params, ma_params)
ma1_data = ma1_process.generate_sample(nsample=n)

df_ma1 = pd.DataFrame({'Time': np.arange(1, n+1), 'Value': ma1_data})
plt.figure(figsize=(8, 4))
plt.plot(df_ma1['Time'], df_ma1['Value'], linewidth=1)
plt.title("MA(1) Model")
plt.xlabel("Time")
plt.ylabel("Value")
plt.tight_layout()
plt.show()
```

### AR(1) with Trend

```{python}
np.random.seed(123)
alpha = 0.5  # Intercept
beta = 0.1   # Slope (trend)
phi = 0.7    # AR coefficient

time = np.arange(1, n+1)
ar1_with_trend = alpha + beta * time + ArmaProcess(np.array([1, -phi]), np.array([1])).generate_sample(nsample=n)

df_trend = pd.DataFrame({'Time': time, 'Value': ar1_with_trend})
plt.figure(figsize=(8, 4))
plt.plot(df_trend['Time'], df_trend['Value'], linewidth=1)
plt.title("AR(1) with Trend Model")
plt.xlabel("Time")
plt.ylabel("Value")
plt.tight_layout()
plt.show()
```

### AR(1) Model with a Structural Break

```{python}
np.random.seed(123)
n_half = n // 2

# First half with AR coefficient 0.8
ar1_before = ArmaProcess(np.array([1, -0.8]), np.array([1])).generate_sample(nsample=n_half)

# Second half with AR coefficient -0.5
ar1_after = ArmaProcess(np.array([1, 0.5]), np.array([1])).generate_sample(nsample=n_half)

# Combine the two halves
time_series_break = np.concatenate([ar1_before, ar1_after])
df_break = pd.DataFrame({'Time': np.arange(1, n+1), 'Value': time_series_break})

plt.figure(figsize=(8, 4))
plt.plot(df_break['Time'], df_break['Value'], linewidth=1)
plt.axvline(x=n_half, linestyle='dashed', color='red')
plt.title("AR(1) Model with Structural Break")
plt.xlabel("Time")
plt.ylabel("Value")
plt.tight_layout()
plt.show()
```

---

## Differencing to Remove Trend

A common transformation is taking first differences:
\[
y_t^* = y_t - y_{t-1}
\]

```{python}
np.random.seed(123)
beta = 0.1  # Trend coefficient
phi = 0.7   # AR coefficient

# Simulate AR(1) with trend
ar1_trend = ArmaProcess(np.array([1, -phi]), np.array([1])).generate_sample(nsample=n) + beta * np.arange(1, n+1)
diff_series = np.diff(ar1_trend)

df_diff = pd.DataFrame({'Time': np.arange(1, n+1), 'Value': ar1_trend})
df_diff['Difference'] = np.concatenate([[np.nan], diff_series])

plt.figure(figsize=(8, 4))
plt.plot(df_diff['Time'], df_diff['Difference'], linewidth=1)
plt.title("AR(1) Model with Trend: First Differences")
plt.xlabel("Time")
plt.ylabel("Difference")
plt.tight_layout()
plt.show()
```

---

## Identification

Assuming that the process is stationary, the three basic types of models are:

- **Autoregressive (AR)**:  
  \[
  y_t = a + b_1 y_{t-1} + b_2 y_{t-2} + \dots + b_p y_{t-p} + \epsilon_t
  \]

- **Moving Average (MA)**:  
  \[
  y_t = \mu + \phi_1 \epsilon_{t-1} + \phi_2 \epsilon_{t-2} + \dots + \phi_q \epsilon_{t-q} + \epsilon_t
  \]

- **Combined ARMA**:  
  \[
  y_t = a + b_1 y_{t-1} + \dots + b_p y_{t-p} + \phi_1 \epsilon_{t-1} + \dots + \phi_q \epsilon_{t-q} + \epsilon_t
  \]

Here, \(\epsilon_t\) is a white noise disturbance:
- \(E(\epsilon_t) = 0\)
- \(Var(\epsilon_t) = \sigma^2\)
- \(Cov(\epsilon_t, \epsilon_s) = 0\) for \(t \neq s\)

---

## Tools for Identification

Key tools include:

- **Visual Inspection** of the series  
- **ACF (Autocorrelation Function)**:  
  \(\rho_j = \gamma_j / \gamma_0\)
- **PACF (Partial Autocorrelation Function)**
  
These tools help choose a candidate model, which you later refine using estimation criteria (AIC, BIC) and residual diagnostics.

---

## ACF and PACF Plots

We use statsmodels’ plotting functions to compute and visualize these functions.

### Example: ACF & PACF for AR(1)

```{python}
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

# Using our previously simulated AR(1) data (from df_ar1)
plt.figure(figsize=(8, 3))
plot_acf(ar1_data, lags=20, zero=False)
plt.title("ACF of AR(1) Model")
plt.tight_layout()
plt.show()

plt.figure(figsize=(8, 3))
plot_pacf(ar1_data, lags=20, zero=False, method='ywm')
plt.title("PACF of AR(1) Model")
plt.tight_layout()
plt.show()
```

### Example: ACF & PACF for MA(1)

```{python}
plt.figure(figsize=(8, 3))
plot_acf(ma1_data, lags=20, zero=False)
plt.title("ACF of MA(1) Model")
plt.tight_layout()
plt.show()

plt.figure(figsize=(8, 3))
plot_pacf(ma1_data, lags=20, zero=False, method='ywm')
plt.title("PACF of MA(1) Model")
plt.tight_layout()
plt.show()
```

### Example: Correlogram for an AR(3) Model

```{python}
# Simulate AR(3) data with coefficients [1.4, -0.6, 0.1]
ar_params_ar3 = np.array([1, -1.4, 0.6, -0.1])
ma_params_ar3 = np.array([1])
ar3_process = ArmaProcess(ar_params_ar3, ma_params_ar3)
ar3_data = ar3_process.generate_sample(nsample=n)

plt.figure(figsize=(8, 3))
plot_acf(ar3_data, lags=20, zero=False)
plt.title("ACF of AR(3) Model")
plt.tight_layout()
plt.show()
```

---

## PACF Patterns

Below are examples for PACF plots with different AR(1) coefficients.

```{python}
# AR(1) with coefficient 0.8
ar1_data_pos = ArmaProcess(np.array([1, -0.8]), np.array([1])).generate_sample(nsample=100)
plt.figure(figsize=(8, 3))
plot_pacf(ar1_data_pos, lags=20, zero=False, method='ywm')
plt.title("PACF of AR(1) Model (phi=0.8)")
plt.tight_layout()
plt.show()

# AR(1) with coefficient -0.8
ar1_data_neg = ArmaProcess(np.array([1, 0.8]), np.array([1])).generate_sample(nsample=100)
plt.figure(figsize=(8, 3))
plot_pacf(ar1_data_neg, lags=20, zero=False, method='ywm')
plt.title("PACF of AR(1) Model (phi=-0.8)")
plt.tight_layout()
plt.show()
```

Similarly, you can simulate and plot PACF for AR(3) or MA(3) models as needed.

---

## Working with Real-World Data

Suppose we have a CSV file `pe.csv` that contains a date column and a column `pe_usa`.

```{python}
# Import libraries
import pandas as pd

# Load data (adjust the path as needed)
pe = pd.read_csv("docs/data/pe.csv")
print(pe.info())

# Select and plot the USA series
pe_us = pe[['date', 'pe_usa']]
pe_us['date'] = pd.to_datetime(pe_us['date'])
plt.figure(figsize=(8, 4))
plt.plot(pe_us['date'], pe_us['pe_usa'])
plt.title("pe_usa over Time")
plt.xlabel("Date")
plt.ylabel("pe_usa")
plt.tight_layout()
plt.show()
```

### Identification with ACF/PACF

```{python}
plt.figure(figsize=(8, 3))
plot_acf(pe_us['pe_usa'], lags=20, zero=False)
plt.title("ACF of pe_usa")
plt.tight_layout()
plt.show()

plt.figure(figsize=(8, 3))
plot_pacf(pe_us['pe_usa'], lags=20, zero=False, method='ywm')
plt.title("PACF of pe_usa")
plt.tight_layout()
plt.show()

# Often the ACF of the raw series suggests nonstationarity.
# One can difference the series and then re-check:
diff_pe = pe_us['pe_usa'].diff().dropna()
plt.figure(figsize=(8, 3))
plot_acf(diff_pe, lags=20, zero=False)
plt.title("ACF of Differenced pe_usa")
plt.tight_layout()
plt.show()

plt.figure(figsize=(8, 3))
plot_pacf(diff_pe, lags=20, zero=False, method='ywm')
plt.title("PACF of Differenced pe_usa")
plt.tight_layout()
plt.show()
```

Based on these plots, you might decide on an ARIMA model (for example, ARIMA(0,1,1) is common when the differenced series shows a significant MA(1) pattern). You can also compare your chosen model with an automatic model selection procedure using the `pmdarima` package:

```{python}
import pmdarima as pm

# Auto-ARIMA model on the pe_usa series
model = pm.auto_arima(pe_us['pe_usa'], seasonal=False, stepwise=True, suppress_warnings=True)
print(model.summary())
```

---

## Nonstationary Series

### What is Nonstationarity?

Recall that for a covariance-stationary series, the mean, variance, and autocovariance are time-invariant. If any of these conditions fail, the series is nonstationary. Common features include:
- No long-run mean (no equilibrium)
- Time-dependent variance (which may increase over time)
- Slowly decaying autocorrelations

A nonstationary series may include a **deterministic trend**:
\[
y_t = \mu + \beta t + u_t \quad \text{(with \(u_t\) i.i.d.)}
\]
For example, with \(\beta = 0.45\):

```{python}
np.random.seed(1269)
n_trend = 100
trend = 0.45 * np.arange(1, n_trend+1)
noise = np.random.normal(loc=0, scale=1, size=n_trend)
yt = trend + noise

df_trend_example = pd.DataFrame({'Time': np.arange(1, n_trend+1), 'yt': yt})
plt.figure(figsize=(8, 4))
plt.plot(df_trend_example['Time'], df_trend_example['yt'])
plt.title("Time Series with Deterministic Trend")
plt.xlabel("Time")
plt.ylabel("yt")
plt.tight_layout()
plt.show()
```

---

## Summary

- We simulated AR and MA models using Python.
- We visualized data with Matplotlib.
- We computed and plotted ACF and PACF using statsmodels.
- We demonstrated how to work with real-world data using Pandas.
- Finally, we touched upon model identification and nonstationarity issues.

This Python version in VS Code (or as a Jupyter Notebook) reproduces the logic and visualizations of your original R/reveal.js presentation.

Happy forecasting!

